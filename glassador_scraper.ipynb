{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in f:\\siika\\python\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: trio~=0.17 in f:\\siika\\python\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in f:\\siika\\python\\lib\\site-packages (from selenium) (2022.6.15)\n",
      "Collecting urllib3[socks]~=1.26\n",
      "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "     ------------------------------------ 140.6/140.6 kB 214.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: trio-websocket~=0.9 in f:\\siika\\python\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: idna in f:\\siika\\python\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: attrs>=19.2.0 in f:\\siika\\python\\lib\\site-packages (from trio~=0.17->selenium) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.14 in f:\\siika\\python\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in f:\\siika\\python\\lib\\site-packages (from trio~=0.17->selenium) (1.0.1)\n",
      "Requirement already satisfied: sniffio in f:\\siika\\python\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: async-generator>=1.9 in f:\\siika\\python\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: sortedcontainers in f:\\siika\\python\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in f:\\siika\\python\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in f:\\siika\\python\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in f:\\siika\\python\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in f:\\siika\\python\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in f:\\siika\\python\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Installing collected packages: urllib3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.23\n",
      "    Uninstalling urllib3-1.23:\n",
      "      Successfully uninstalled urllib3-1.23\n",
      "Successfully installed urllib3-1.26.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (f:\\siika\\python\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\siika\\python\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\siika\\python\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -ip (f:\\siika\\python\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\siika\\python\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "jupyterlab-server 2.15.1 requires jinja2>=3.0.3, but you have jinja2 2.11.3 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\siika\\python\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\siika\\python\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (f:\\siika\\python\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "from shutil import which\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException, ElementNotInteractableException\n",
    "import time\n",
    "from lxml import html\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch jobs function is about looping on the job cards in a job search and collect the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_name = []\n",
    "job_title = []\n",
    "salary_est = []\n",
    "location = []\n",
    "job_description = []\n",
    "salary_estimate = []\n",
    "company_size = []\n",
    "company_type = []\n",
    "company_sector = []\n",
    "company_industry = []\n",
    "company_founded = []\n",
    "company_revenue = []\n",
    "def fetch_jobs(keyword, num_pages,path):\n",
    "    options = Options()\n",
    "    options.add_argument(\"window-size=1920,1080\")\n",
    "    #Enter your chromedriver.exe path below\n",
    "    chrome_path= path\n",
    "    driver = webdriver.Chrome(executable_path=chrome_path, options=options)\n",
    "    url = 'https://www.glassdoor.com/Job/jobs.htm?sc.keyword=\"' + keyword + '\"&locT=C&locId=1147401&locKeyword=San%20Francisco,%20CA&jobType=all&fromAge=-1&minSalary=0&includeNoSalaryJobs=true&radius=100&cityId=-1&minRating=0.0&industryId=-1&sgocId=-1&seniorityType=all&companyId=-1&employerSizes=0&applicationType=0&remoteWorkType=0'\n",
    "\n",
    "    driver.get(url)\n",
    "  \n",
    "    time.sleep(2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Set current page to 1\n",
    "    current_page = 1     \n",
    "        \n",
    "        \n",
    "    time.sleep(3)\n",
    "    \n",
    "    while current_page <= num_pages:   \n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            job_cards = driver.find_elements(By.XPATH,\"//article[@id='MainCol']//ul/li[@data-adv-type='GENERAL']\")\n",
    "            for card in job_cards:\n",
    "                card.click()\n",
    "                time.sleep(1)\n",
    "\n",
    "                #Closes the signup prompt\n",
    "                try:\n",
    "                    driver.find_element(By.XPATH,\".//span[@class='SVGInline modal_closeIcon']\").click()\n",
    "                    time.sleep(1)\n",
    "                except:\n",
    "                    time.sleep(2)\n",
    "                    pass\n",
    "\n",
    "                #Expands the Description section by clicking on Show More\n",
    "                opened=False\n",
    "                tries=0\n",
    "                while not opened and tries!=5:\n",
    "                    try:\n",
    "                        driver.find_element(By.XPATH,\"//div[@class='css-t3xrds e856ufb4']\").click()\n",
    "                        time.sleep(1)\n",
    "                        opened=True\n",
    "                    except NoSuchElementException:\n",
    "                        card.click()\n",
    "                        print(str(current_page) + '#ERROR: no such element')\n",
    "                        tries+=1\n",
    "                        time.sleep(2)\n",
    "                    \n",
    "                #Scrape \n",
    "\n",
    "                try:\n",
    "                    company_name.append(driver.find_element(By.XPATH,\"//div[@class='css-87uc0g e1tk4kwz1']\").text)\n",
    "                except:\n",
    "                    company_name.append(\"#N/A\")\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    job_title.append(driver.find_element(By.XPATH,\"//div[@class='css-1vg6q84 e1tk4kwz4']\").text)\n",
    "                except:\n",
    "                    job_title.append(\"#N/A\")\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    location.append(driver.find_element(By.XPATH,\"//div[@class='css-56kyx5 e1tk4kwz5']\").text)\n",
    "                except:\n",
    "                    location.append(\"#N/A\")\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    job_description.append(driver.find_element(By.XPATH,\"//div[@id='JobDescriptionContainer']\").text)\n",
    "                except:\n",
    "                    job_description.append(\"#N/A\")\n",
    "                    pass\n",
    "\n",
    "                try:\n",
    "                    salary_estimate.append(driver.find_element(By.XPATH,\"//div[@class='css-1bluz6i e2u4hf13']\").text)\n",
    "                except:\n",
    "                    salary_estimate.append(\"#N/A\")\n",
    "                    pass\n",
    "                \n",
    "                try:\n",
    "                    company_size.append(driver.find_element(By.XPATH,\"//div[@id='CompanyContainer']//span[text()='Size']//following-sibling::*\").text)\n",
    "                except:\n",
    "                    company_size.append(\"#N/A\")\n",
    "                    pass\n",
    "                \n",
    "                try:\n",
    "                    company_type.append(driver.find_element(By.XPATH,\"//div[@id='CompanyContainer']//span[text()='Type']//following-sibling::*\").text)\n",
    "                except:\n",
    "                    company_type.append(\"#N/A\")\n",
    "                    pass\n",
    "                    \n",
    "                try:\n",
    "                    company_sector.append(driver.find_element(By.XPATH,\"//div[@id='CompanyContainer']//span[text()='Sector']//following-sibling::*\").text)\n",
    "                except:\n",
    "                    company_sector.append(\"#N/A\")\n",
    "                    pass\n",
    "                    \n",
    "                try:\n",
    "                    company_industry.append(driver.find_element(By.XPATH,\"//div[@id='CompanyContainer']//span[text()='Industry']//following-sibling::*\").text)\n",
    "                except:\n",
    "                    company_industry.append(\"#N/A\")\n",
    "                    pass\n",
    "                    \n",
    "                try:\n",
    "                    company_founded.append(driver.find_element(By.XPATH,\"//div[@id='CompanyContainer']//span[text()='Founded']//following-sibling::*\").text)\n",
    "                except:\n",
    "                    company_founded.append(\"#N/A\")\n",
    "                    pass\n",
    "                    \n",
    "                try:\n",
    "                    company_revenue.append(driver.find_element(By.XPATH,\"//div[@id='CompanyContainer']//span[text()='Revenue']//following-sibling::*\").text)\n",
    "                except:\n",
    "                    company_revenue.append(\"#N/A\")\n",
    "                    pass\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                done = True\n",
    "                \n",
    "       # Moves to the next page         \n",
    "        if done:\n",
    "            print(str(current_page) + ' ' + 'out of' +' '+ str(num_pages) + ' ' + 'pages done')\n",
    "            driver.find_element(By.XPATH,\"//span[@alt='next-icon']\").click()   \n",
    "            current_page = current_page + 1\n",
    "            time.sleep(4)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    driver.close()\n",
    "    df = pd.DataFrame({'company': company_name, \n",
    "    'job title': job_title,\n",
    "    'location': location,\n",
    "    'job description': job_description,\n",
    "    'salary estimate': salary_estimate,\n",
    "    'company_size': company_size,\n",
    "    'company_type': company_type,\n",
    "    'company_sector': company_sector,\n",
    "    'company_industry' : company_industry, 'company_founded' : company_founded, 'company_revenue': company_revenue})\n",
    "    \n",
    "    df.to_csv(keyword+ '.csv')\n",
    "                       \n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msiik\\AppData\\Local\\Temp\\ipykernel_23668\\4217251336.py:18: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=chrome_path, options=options)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1 out of 10 pages done\n",
      "2#ERROR: no such element\n",
      "2#ERROR: no such element\n",
      "2#ERROR: no such element\n",
      "2#ERROR: no such element\n",
      "2 out of 10 pages done\n",
      "3 out of 10 pages done\n",
      "4 out of 10 pages done\n",
      "5#ERROR: no such element\n",
      "5#ERROR: no such element\n",
      "5#ERROR: no such element\n",
      "5 out of 10 pages done\n",
      "6#ERROR: no such element\n",
      "6#ERROR: no such element\n",
      "6 out of 10 pages done\n",
      "7#ERROR: no such element\n",
      "7#ERROR: no such element\n",
      "7#ERROR: no such element\n",
      "7#ERROR: no such element\n",
      "7 out of 10 pages done\n",
      "8#ERROR: no such element\n",
      "8#ERROR: no such element\n",
      "8 out of 10 pages done\n",
      "9#ERROR: no such element\n",
      "9#ERROR: no such element\n",
      "9#ERROR: no such element\n",
      "9 out of 10 pages done\n",
      "10#ERROR: no such element\n",
      "10 out of 10 pages done\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1 out of 10 pages done\n",
      "2#ERROR: no such element\n",
      "2#ERROR: no such element\n",
      "2 out of 10 pages done\n",
      "3#ERROR: no such element\n",
      "3#ERROR: no such element\n",
      "3#ERROR: no such element\n",
      "3 out of 10 pages done\n",
      "4#ERROR: no such element\n",
      "4 out of 10 pages done\n",
      "5#ERROR: no such element\n",
      "5 out of 10 pages done\n",
      "6 out of 10 pages done\n",
      "7#ERROR: no such element\n",
      "7 out of 10 pages done\n",
      "8#ERROR: no such element\n",
      "8 out of 10 pages done\n",
      "9#ERROR: no such element\n",
      "9#ERROR: no such element\n",
      "9#ERROR: no such element\n",
      "9 out of 10 pages done\n",
      "10#ERROR: no such element\n",
      "10 out of 10 pages done\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1#ERROR: no such element\n",
      "1 out of 10 pages done\n",
      "2#ERROR: no such element\n",
      "2#ERROR: no such element\n",
      "2 out of 10 pages done\n",
      "3#ERROR: no such element\n",
      "3 out of 10 pages done\n",
      "4#ERROR: no such element\n",
      "4#ERROR: no such element\n",
      "4#ERROR: no such element\n",
      "4 out of 10 pages done\n",
      "5#ERROR: no such element\n",
      "5#ERROR: no such element\n",
      "5#ERROR: no such element\n",
      "5 out of 10 pages done\n",
      "6#ERROR: no such element\n",
      "6#ERROR: no such element\n",
      "6 out of 10 pages done\n",
      "7 out of 10 pages done\n",
      "8#ERROR: no such element\n",
      "8#ERROR: no such element\n",
      "8 out of 10 pages done\n",
      "9#ERROR: no such element\n",
      "9 out of 10 pages done\n",
      "10#ERROR: no such element\n",
      "10#ERROR: no such element\n",
      "10#ERROR: no such element\n",
      "10#ERROR: no such element\n",
      "10#ERROR: no such element\n",
      "10#ERROR: no such element\n",
      "10#ERROR: no such element\n",
      "10#ERROR: no such element\n",
      "10 out of 10 pages done\n"
     ]
    }
   ],
   "source": [
    "chromepath=\"F:/siika/repos/Jobs_salary_Prediction/chromedriver\"\n",
    "fetch_jobs(\"Data Engineer\",10,chromepath)\n",
    "fetch_jobs(\"Data Analyst\",10,chromepath)\n",
    "fetch_jobs(\"Machine Learning\",10,chromepath)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one for scraping a certain job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.glassdoor.com/job-listing/sr-data-scientist-f5-JV_IC1138644_KO0,17_KE18,20.htm?jl=1007135496738&pos=203&ao=1136043&s=58&guid=0000018597df5e3091fbc0b92fa696cb&src=GD_JOB_AD&t=SR&vt=w&uido=C9591E0F2E81C40321839CEF58B3CF51&cs=1_46e8ccbf&cb=1673290276769&jobListingId=1007135496738&jrtk=3-0-1gmbtunj7kf3e801-1gmbtunkb2i72000-8023fc3b0c0cf96c-&ctt=1673290320897\"\n",
    "def scrape_one_job(job_card_url,path):\n",
    "    options = Options()\n",
    "    options.add_argument(\"window-size=1920,1080\")\n",
    "    #Enter your chromedriver.exe path below\n",
    "    chrome_path = path\n",
    "    driver = webdriver.Chrome(executable_path=chrome_path, options=options)\n",
    "  # Navigate to the job card page\n",
    "    driver.get(job_card_url)    \n",
    "    # Wait for the page to load\n",
    "    driver.implicitly_wait(3)\n",
    "    \n",
    "    opened=False\n",
    "    while not opened:\n",
    "        try:\n",
    "            driver.find_element(By.XPATH,\"//div[@class='css-1rzz8ht ecgq1xb2']\").click()\n",
    "            time.sleep(1)\n",
    "            opened=True\n",
    "        except NoSuchElementException:\n",
    "            print('#ERROR: no such element')\n",
    "            time.sleep(2)    \n",
    "    # Extract the data for the job\n",
    "    try:\n",
    "        company_name=(driver.find_element(By.XPATH,\"//div[@class='css-16nw49e e11nt52q1']\").text)\n",
    "    except:\n",
    "        company_name=(\"#N/A\")\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        job_title=(driver.find_element(By.XPATH,\"//div[@class='css-17x2pwl e11nt52q6']\").text)\n",
    "    except:\n",
    "        job_title=(\"#N/A\")\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        location=(driver.find_element(By.XPATH,\"//div[@class='css-1v5elnn e11nt52q2']\").text)\n",
    "    except:\n",
    "        location=(\"#N/A\")\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        job_description=(driver.find_element(By.XPATH,\"//div[@id='JobDescriptionContainer']\").text)\n",
    "    except:\n",
    "        job_description=(\"#N/A\")\n",
    "        pass\n",
    "\n",
    "    driver.find_element(By.XPATH,\"//div[@class='css-1iqg1r5 e1eh6fgm0']//span[text()='Company']\").click()\n",
    "    driver.implicitly_wait(3)\n",
    "\n",
    "    \n",
    "    try:\n",
    "        company_size=(driver.find_element(By.XPATH,\"//div[@class='css-vugejy es5l5kg0']//label[text()='Size']//following-sibling::*\").text)\n",
    "    except:\n",
    "        company_size=(\"#N/A\")\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        company_type=(driver.find_element(By.XPATH,\"//div[@class='css-vugejy es5l5kg0']//label[text()='Type']//following-sibling::*\").text)\n",
    "    except:\n",
    "        company_type=(\"#N/A\")\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        company_sector=(driver.find_element(By.XPATH,\"//div[@class='css-vugejy es5l5kg0']//label[text()='Sector']//following-sibling::*\").text)\n",
    "    except:\n",
    "        company_sector=(\"#N/A\")\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        company_industry=(driver.find_element(By.XPATH,\"//div[@class='css-vugejy es5l5kg0']//label[text()='Industry']//following-sibling::*\").text)\n",
    "    except:\n",
    "        company_industry=(\"#N/A\")\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        company_founded=(driver.find_element(By.XPATH,\"//div[@class='css-vugejy es5l5kg0']//label[text()='Founded']//following-sibling::*\").text)\n",
    "    except:\n",
    "        company_founded=(\"#N/A\")\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        company_revenue=(driver.find_element(By.XPATH,\"//div[@class='css-vugejy es5l5kg0']//label[text()='Revenue']//following-sibling::*\").text)\n",
    "    except:\n",
    "        company_revenue=(\"#N/A\")\n",
    "        pass\n",
    "\n",
    "    # Save the data for the job\n",
    "    job = {\n",
    "      'company_name': company_name,\n",
    "      'job_title': job_title,\n",
    "      'location': location,\n",
    "      'job_description': job_description,\n",
    "      'company_size': company_size,\n",
    "      'company_type': company_type,\n",
    "      'company_sector': company_sector,\n",
    "      'company_industry': company_industry,\n",
    "      'company_founded': company_founded,\n",
    "      'company_revenue': company_revenue\n",
    "    }\n",
    "    \n",
    "    \n",
    "  # Close the webdriver\n",
    "    driver.quit()\n",
    "\n",
    "    return job\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msiik\\AppData\\Local\\Temp\\ipykernel_23668\\190989400.py:7: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=chrome_path, options=options)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'company_name': 'f5\\n4.2\\n★',\n",
       " 'job_title': 'Sr Data Scientist',\n",
       " 'location': 'Charlotte, NC',\n",
       " 'job_description': \"At F5, we strive to bring a better digital world to life. Our teams empower organizations across the globe to create, secure, and run applications that enhance how we experience our evolving digital world. We are passionate about cybersecurity, from protecting consumers from fraud to enabling companies to focus on innovation.\\n\\nEverything we do centers around people. That means we obsess over how to make the lives of our customers, and their customers, better. And it means we prioritize a diverse F5 community where each individual can thrive.\\nToday, as part of F5, Security products identify and stop automated attacks and fraud against web and mobile applications of the Fortune Global 2000. Tomorrow, we’ll need to fight even more sophisticated adversaries, while helping human users to have ever more friction-less experience. Come be a part of our unparalleled team that is responsible for making the Internet a safer place for everyone.\\nThe Position\\nWE are seeking a driven, analytical and highly professional data scientist to help deliver our anti-fraud solution to customers, and the future of what we build. You will enjoy working with one of the richest data sets in the world, cutting-edge technology, and the opportunity to train and deploy machine learning models to fight fraud, in real-time and on highly visible web and mobile applications.\\nThis Data Scientist will be part of a team that is responsible for delivering F5's anti-fraud solution to customers. You’ll have an opportunity to engage closely with customers, and become an expert in fraud across multiple use cases and industry verticals. We are looking for a data scientist who has a nose for identifying fraud and anomalous patterns in huge and often messy datasets, and who is adept at using these insights to build high-performance machine learning models. The ideal candidate will also have a penchant for working closely with enterprise customers, and a desire to deliver outstanding value to them.\\nResponsibilities:\\nWork closely with customers to quickly understand their fraud challenges, and identify how F5 might be able to help them successfully fight fraud.\\nIdentify useful data from customers’ web and mobile applications, and engineer relevant features for fraud models.\\nTrain and deploy machine learning models to predict fraud outcomes, based on telemetry and customer-provided data.\\nMaintain models for customers, and update or refresh them as needed.\\nCommunicate clearly and effectively with internal teams (sales, customer success, product, engineering, etc) and with customers on the performance of machine learning models, as well as other fraud and data science issues.\\nProvide feedback to internal teams on data science, product, engineering issues, and suggest process improvements.\\nRequirements:\\nThe ideal candidate would have:\\nStrong analytical skills including the ability to manipulate, model, interpret and visualize large quantities of structured data.\\nStrong proficiency in Python and SQL\\nExperience with building and deploying machine learning models on large-scale data. Experience working with highly unbalanced data (e.g. commonly found in fraud use cases) a strong plus.\\nA passion for delighting customers, and an aptitude for engaging with customers on fraud, security and machine learning issues on a regular basis. Prior experience working with customers on complex fraud or security issues would be a strong plus.\\nFamiliarity with common web and mobile technologies.\\nStrong interpersonal skills, personable, and persistent. A good listener. Self-motivated, able to work well both independently and as part of an agile team.\\nA demonstrated enthusiasm and capacity to learn new technologies quickly.\\nA scrappy yet meticulous approach, and a love for problem solving!\\n#LI-AP1\\n#LI-REMOTE\\nThe Job Description is intended to be a general representation of the responsibilities and requirements of the job. However, the description may not be all-inclusive, and responsibilities and requirements are subject to change.\\nThe annual U.S. base pay range for this position is: $145,908.00 - $218,862.00\\nF5 maintains broad salary ranges for its roles in order to account for variations in knowledge, skills, experience, geographic locations, and market conditions, as well as to reflect F5’s differing products, industries, and lines of business. The pay range referenced is as of the time of the job posting and is subject to change.\\nYou may also be offered incentive compensation, bonus, restricted stock units, and benefits. More details about F5’s benefits can be found at the following link:\\nhttps://www.f5.com/company/careers/benefits\\n. F5 reserves the right to change or terminate any benefit plan without notice.\\nPlease note that F5 only contacts candidates through F5 email address (ending with @f5.com) or auto email notification from Yello/Workday (ending with f5.com or @myworkday.com).\\nEqual Employment Opportunity\\nIt is the policy of F5 to provide equal employment opportunities to all employees and employment applicants without regard to unlawful considerations of race, religion, color, national origin, sex, sexual orientation, gender identity or expression, age, sensory, physical, or mental disability, marital status, veteran or military status, genetic information, or any other classification protected by applicable local, state, or federal laws. This policy applies to all aspects of employment, including, but not limited to, hiring, job assignment, compensation, promotion, benefits, training, discipline, and termination. F5 offers a variety of reasonable accommodations for candidates. Requesting an accommodation is completely voluntary. F5 will assess the need for accommodations in the application process separately from those that may be needed to perform the job. Request by contacting\\naccommodations@f5.com\\n.\\nShow less\\nReport\",\n",
       " 'company_size': '5001 to 10000 Employees',\n",
       " 'company_type': 'Company - Public',\n",
       " 'company_sector': 'Information Technology',\n",
       " 'company_industry': 'Computer Hardware Development',\n",
       " 'company_founded': '1996',\n",
       " 'company_revenue': '$1 to $5 billion (USD)'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chromepath=\"F:/siika/repos/Jobs_salary_Prediction/chromedriver\"\n",
    "scrape_one_job(url,chromepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Index(['company', 'job title', 'location', 'job description',\n",
       "        'salary estimate', 'company_size', 'company_type', 'company_sector',\n",
       "        'company_industry', 'company_founded', 'company_revenue'],\n",
       "       dtype='object'),\n",
       " Index(['company', 'job title', 'location', 'job description',\n",
       "        'salary estimate', 'company_size', 'company_type', 'company_sector',\n",
       "        'company_industry', 'company_founded', 'company_revenue'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_old=pd.read_csv(\"Data Scientist.csv\")\n",
    "df_old.drop(columns=[\"Unnamed: 0\"],axis=1,inplace=True)\n",
    "df_old.columns,df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1698, 11)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df=pd.concat([df_old,df])\n",
    "final_df.to_csv(\"Data Scientist.csv\")\n",
    "final_df.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a2a32ab4a5f7cd90480dbabb09257fae3defe07d2d8fed2a66cce74ba4c49416"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
